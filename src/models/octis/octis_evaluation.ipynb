{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCTIS Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prologue & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the performance of most relevant OCTIS models as a baseline for non-SOTA Topic Modeling. These models will be compared on the same preprocessed dataset, the same number of topics and the same evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstop_words\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STOP_WORDS \u001b[38;5;28;01mas\u001b[39;00m el_stop\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01men\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstop_words\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m STOP_WORDS \u001b[38;5;28;01mas\u001b[39;00m en_stop\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GreekPMDataloader\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moctis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgr_preprocessor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GreekStanzaPreprocessor\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from octis.models import LDA, ETM, NMF, NeuralLDA\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from spacy.lang.el.stop_words import STOP_WORDS as el_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "from utils.data_loader import GreekPMDataloader\n",
    "from models.octis.utils.gr_preprocessor import GreekStanzaPreprocessor\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our dataset has already been processed and cached, then we can load it. Otherwise, we will preprocess it and save it for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2032 entries, 0 to 2031\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   date    2032 non-null   object\n",
      " 1   id      2032 non-null   int64 \n",
      " 2   url     2032 non-null   object\n",
      " 3   title   2032 non-null   object\n",
      " 4   text    2012 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 79.5+ KB\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset = Dataset()\n",
    "    dataset.load_data_from_csv('data/octis/dataset_processed')\n",
    "except:\n",
    "    # Merge data and prepare for preprocessing\n",
    "    try:\n",
    "        speeches_df = pd.read_csv('data/data_speeches.csv')\n",
    "        statements_df = pd.read_csv('data/data_statements.csv')\n",
    "    except: \n",
    "        ds = GreekPMDataloader() # If the data is not available, download it\n",
    "        cats_df = ds.load_categories(\"speeches\", \"statements\")\n",
    "\n",
    "    df = pd.concat([speeches_df, statements_df], ignore_index=True)\n",
    "    \n",
    "    # Drop irrelevant columns and convert to string\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    df = df.drop(columns=['date', 'id', 'url', 'title']).dropna(how='any')\n",
    "    \n",
    "    df.to_csv('data/data_merged.csv', index=False)\n",
    "\n",
    "    # We have some non-Greek stopwords in the dataset, so we need to remove them\n",
    "    stopwords = set(el_stop).union(set(en_stop))\n",
    "    \n",
    "    # Initialize preprocessing\n",
    "    preprocessor = GreekStanzaPreprocessor(\n",
    "                             stopword_list=stopwords,\n",
    "                             remove_punctuation=True, remove_numbers=True,\n",
    "                             max_df=0.2, min_df=0.01, min_chars=4, min_words=20, \n",
    "                             num_processes=6)\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = preprocessor.preprocess_dataset(documents_path='data/data_merged.csv')\n",
    "    \n",
    "    dataset.save('data/octis/dataset_processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralLDA(num_topics=13, num_neurons=200, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\tSamples: [1442/144200]\tTrain Loss: 3599.828114164355\tTime: 0:00:00.114037\n",
      "Epoch: [1/100]\tSamples: [161/16100]\tValidation Loss: 153238703.3591809\tTime: 0:00:00.004465\n",
      "Epoch: [2/100]\tSamples: [2884/144200]\tTrain Loss: 3666.4920087118585\tTime: 0:00:00.115254\n",
      "Epoch: [2/100]\tSamples: [161/16100]\tValidation Loss: 45624.85326086957\tTime: 0:00:00.004576\n",
      "Epoch: [3/100]\tSamples: [4326/144200]\tTrain Loss: 3395.3487235610264\tTime: 0:00:00.109990\n",
      "Epoch: [3/100]\tSamples: [161/16100]\tValidation Loss: 3201.4151785714284\tTime: 0:00:00.004606\n",
      "Epoch: [4/100]\tSamples: [5768/144200]\tTrain Loss: 3372.738015776699\tTime: 0:00:00.105471\n",
      "Epoch: [4/100]\tSamples: [161/16100]\tValidation Loss: 3165.120875388199\tTime: 0:00:00.004396\n",
      "Epoch: [5/100]\tSamples: [7210/144200]\tTrain Loss: 3384.5410996012483\tTime: 0:00:00.105965\n",
      "Epoch: [5/100]\tSamples: [161/16100]\tValidation Loss: 3138.5403726708073\tTime: 0:00:00.004676\n",
      "Epoch: [6/100]\tSamples: [8652/144200]\tTrain Loss: 3338.6350825676145\tTime: 0:00:00.102723\n",
      "Epoch: [6/100]\tSamples: [161/16100]\tValidation Loss: 3136.505337732919\tTime: 0:00:00.005180\n",
      "Epoch: [7/100]\tSamples: [10094/144200]\tTrain Loss: 3330.281130807906\tTime: 0:00:00.152392\n",
      "Epoch: [7/100]\tSamples: [161/16100]\tValidation Loss: 3121.1749320652175\tTime: 0:00:00.004381\n",
      "Epoch: [8/100]\tSamples: [11536/144200]\tTrain Loss: 3307.7407138522885\tTime: 0:00:00.115625\n",
      "Epoch: [8/100]\tSamples: [161/16100]\tValidation Loss: 3118.9073660714284\tTime: 0:00:00.004833\n",
      "Epoch: [9/100]\tSamples: [12978/144200]\tTrain Loss: 3316.2872204403607\tTime: 0:00:00.108120\n",
      "Epoch: [9/100]\tSamples: [161/16100]\tValidation Loss: 3125.9023194875776\tTime: 0:00:00.004092\n",
      "Epoch: [10/100]\tSamples: [14420/144200]\tTrain Loss: 3297.7797330097087\tTime: 0:00:00.110160\n",
      "Epoch: [10/100]\tSamples: [161/16100]\tValidation Loss: 3110.094235248447\tTime: 0:00:00.004712\n",
      "Epoch: [11/100]\tSamples: [15862/144200]\tTrain Loss: 3318.362311459778\tTime: 0:00:00.107726\n",
      "Epoch: [11/100]\tSamples: [161/16100]\tValidation Loss: 3101.3831036490683\tTime: 0:00:00.004762\n",
      "Epoch: [12/100]\tSamples: [17304/144200]\tTrain Loss: 3300.5433805045077\tTime: 0:00:00.106051\n",
      "Epoch: [12/100]\tSamples: [161/16100]\tValidation Loss: 3088.1614906832297\tTime: 0:00:00.005292\n",
      "Epoch: [13/100]\tSamples: [18746/144200]\tTrain Loss: 3294.190999913315\tTime: 0:00:00.109768\n",
      "Epoch: [13/100]\tSamples: [161/16100]\tValidation Loss: 3113.0269798136646\tTime: 0:00:00.006366\n",
      "Epoch: [14/100]\tSamples: [20188/144200]\tTrain Loss: 3326.0749664095006\tTime: 0:00:00.099856\n",
      "Epoch: [14/100]\tSamples: [161/16100]\tValidation Loss: 3095.7832880434785\tTime: 0:00:00.004587\n",
      "Epoch: [15/100]\tSamples: [21630/144200]\tTrain Loss: 3312.6079934552704\tTime: 0:00:00.108634\n",
      "Epoch: [15/100]\tSamples: [161/16100]\tValidation Loss: 3120.2197204968943\tTime: 0:00:00.004543\n",
      "Epoch: [16/100]\tSamples: [23072/144200]\tTrain Loss: 3290.8088538054785\tTime: 0:00:00.105460\n",
      "Epoch: [16/100]\tSamples: [161/16100]\tValidation Loss: 3110.6464479813662\tTime: 0:00:00.005092\n",
      "Epoch: [17/100]\tSamples: [24514/144200]\tTrain Loss: 3258.30855040742\tTime: 0:00:00.101358\n",
      "Epoch: [17/100]\tSamples: [161/16100]\tValidation Loss: 3104.1714868012423\tTime: 0:00:00.004327\n",
      "Early stopping\n",
      "topics\n",
      "topic-document-matrix\n",
      "topic-word-matrix\n",
      "test-topic-document-matrix\n"
     ]
    }
   ],
   "source": [
    "output = model.train_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['topics', 'topic-document-matrix', 'topic-word-matrix', 'test-topic-document-matrix'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "εκλογή μνημόνιο χιλιάδα θυσία φόρος ανεργία λάθος πλεόνασμα ψηφίζω αντιπολίτευση\n",
      "υφυπουργός σύσκεψη εκτιμώ παραπληροφόρηση ταχύτητα ξεχωριστός γρήγορος επίπτωση καλοκαίρι εσωτερικά\n",
      "δημοσιονομικός οργανισμός έτοιμος συμμετοχή επανέρχομαι πρόσβαση παρέχω διαμορφώνω ενεργειακός ερώτηση\n",
      "χέρι οργάνωση μηχανισμός συγκεκριμένα μοναδικός εύκολος χρονικός πρακτικός χρηματοδοτώ ικανός\n",
      "εξαιρετικός παραπληροφόρηση υποδέχομαι συγχαρητήριο λαμβάνω γεωπολιτικός καταστροφικά μετάβαση δημιουργικότητα συνομιλώ\n",
      "τουρισμός παραγωγή παραγωγικός πανεπιστήμιο δημοκρατικός λάθος δικαιοσύνη τεχνολογία διαπραγμάτευση βουλευτής\n",
      "δολοφονία υπεραξία μνημόνια μητροπολίτης βρετανία σλοβενία βέλγιο χριστούγεννα θράκη διατίθεμαι\n",
      "that also have with much will about very τεχνολογία εμπειρία\n",
      "προσωπικό νοσοκομείο μονάδα νομοσχέδιο δικαιοσύνη διαφορά δρομολογώ κρίνω αποκτάω αίθουσα\n",
      "παραγωγή προϋπόθεση ανταγωνιστικότητα τρίτος δίκτυο δουλεύω αγροτικός περιμένω προϋπολογισμός δισεκατομμύριο\n",
      "μετατρέπω σύνολο περιγράφω κομμάτι περιορίζω δομή χαρακτηρίζω συντονισμός επαφή σκοπός\n",
      "έναρξη εμβολιασμός μετανάστευση κορυφή θερμός εξωτερικά πρόσφατος προσερχόμενος ακόλουθος μνημόνια\n",
      "πρίσμα θεωρητικός υστέρηση πανελλαδικός προσερχόμενος θράκη μεσημέρι θέρω μνημόνια μητροπολίτης\n"
     ]
    }
   ],
   "source": [
    "for t in output['topics']:\n",
    "  print(\" \".join(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "npmi = Coherence(texts=dataset.get_corpus(), topk=5, measure='c_npmi')\n",
    "cv = Coherence(texts=dataset.get_corpus(), topk=5, measure='c_v')\n",
    "umass = Coherence(texts=dataset.get_corpus(), topk=5, measure='u_mass')\n",
    "uci = Coherence(texts=dataset.get_corpus(), topk=5, measure='c_uci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_diversity = TopicDiversity(topk=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic diversity: 0.9692307692307692\n",
      "NPMI Coherence: -0.06526148338418904\n",
      "C_V Coherence: 0.5432437733522696\n",
      "U_MASS Coherence: -3.643577635249891\n",
      "C_UCI Coherence: -2.798853319611031\n"
     ]
    }
   ],
   "source": [
    "topic_diversity_score = topic_diversity.score(output)\n",
    "print(\"Topic diversity: \"+str(topic_diversity_score))\n",
    "\n",
    "npmi_score = npmi.score(output)\n",
    "print(\"NPMI Coherence: \"+str(npmi_score)) \n",
    "\n",
    "cv_score = cv.score(output)\n",
    "print(\"C_V Coherence: \"+str(cv_score))\n",
    "\n",
    "umass_score = umass.score(output)\n",
    "print(\"U_MASS Coherence: \"+str(umass_score))\n",
    "\n",
    "uci_score = uci.score(output)\n",
    "print(\"C_UCI Coherence: \"+str(uci_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
