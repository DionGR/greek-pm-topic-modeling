{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCTIS Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prologue & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the performance of most relevant OCTIS models as a baseline for non-SOTA Topic Modeling. These models will be compared on the same preprocessed dataset, the same number of topics and the same evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 13:29:10 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800fa4bedd794d1f8822257a6943d273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-04 13:29:10 INFO: Downloaded file to /Users/dion/stanza_resources/resources.json\n",
      "2024-04-04 13:29:10 WARNING: Language el package default expects mwt, which has been added\n",
      "2024-04-04 13:29:11 INFO: Loading these models for language: el (Greek):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | gdt                     |\n",
      "| mwt       | gdt                     |\n",
      "| pos       | models/oct..._tagger.pt |\n",
      "| lemma     | models/oct...matizer.pt |\n",
      "=======================================\n",
      "\n",
      "2024-04-04 13:29:11 INFO: Using device: cpu\n",
      "2024-04-04 13:29:11 INFO: Loading: tokenize\n",
      "2024-04-04 13:29:11 INFO: Loading: mwt\n",
      "2024-04-04 13:29:11 INFO: Loading: pos\n",
      "2024-04-04 13:29:11 INFO: Loading: lemma\n",
      "2024-04-04 13:29:11 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from octis.models.LSI import LSI\n",
    "from octis.models.NMF import NMF\n",
    "from octis.models.LDA import LDA\n",
    "from octis.models.HDP import HDP\n",
    "from octis.models.NeuralLDA import NeuralLDA\n",
    "from octis.models.ProdLDA import ProdLDA\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity, KLDivergence\n",
    "from octis.evaluation_metrics.similarity_metrics import RBO, PairwiseJaccardSimilarity\n",
    "from octis.evaluation_metrics.topic_significance_metrics import KL_uniform\n",
    "\n",
    "from spacy.lang.el.stop_words import STOP_WORDS as el_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "from utils.data_loader import GreekPMDataloader\n",
    "from models.octis.utils.preprocessor_gr import GreekStanzaPreprocessor\n",
    "from models.octis.config.preprocessing import preprocessor_gr_params\n",
    "from models.octis.config.models import NUM_TOPICS, lsi_params, nmf_params, lda_params, hdp_params, neural_lda_params, prod_lda_params\n",
    "from models.octis.config.optimization import OPTIMIZATION_RESULT_PATH, TOP_K, NUM_PROCESSES, MODEL_RUNS, search_space\n",
    "from models.octis.utils.model_evaluator import OCTISModelEvaluator\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our dataset has already been processed and cached, then we can load it. Otherwise, we will preprocess it and save it for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset found cached - loading...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset = Dataset()\n",
    "    dataset.load_custom_dataset_from_folder('models/octis/data/dataset')\n",
    "    print(\"Dataset found cached - loading...\")\n",
    "except:\n",
    "    print(\"Dataset not found in cache - loading...\")\n",
    "    # Merge data and prepare for preprocessing\n",
    "    try:\n",
    "        speeches_df = pd.read_csv('data/data_speeches.csv')\n",
    "        statements_df = pd.read_csv('data/data_statements.csv')\n",
    "    except: \n",
    "        print(\"GreekPM data not found - fetching...\")\n",
    "        ds = GreekPMDataloader() # If the data is not available, download it\n",
    "        cats_df = ds.load_categories(\"speeches\", \"statements\")\n",
    "        print(\"GreekPM data fetched!\")\n",
    "\n",
    "    df = pd.concat([speeches_df, statements_df], ignore_index=True)\n",
    "    \n",
    "    # Drop irrelevant columns and convert to string\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    df = df.drop(columns=['date', 'id', 'url', 'title']).dropna(how='any')\n",
    "    \n",
    "    df.to_csv('data/data_merged.csv', index=False)\n",
    "\n",
    "    # We have some non-Greek stopwords in the dataset, so we need to remove them\n",
    "    stopwords = set(el_stop).union(set(en_stop))\n",
    "    \n",
    "    # Initialize preprocessing\n",
    "    preprocessor = GreekStanzaPreprocessor(\n",
    "                             stopword_list=stopwords, \n",
    "                             **preprocessor_gr_params)\n",
    "    \n",
    "    # Create the dataset\n",
    "    print(\"Preprocessing data...\")\n",
    "    dataset = preprocessor.preprocess_dataset(documents_path='data/data_merged.csv')\n",
    "    \n",
    "    dataset.save('models/octis/data/dataset/')\n",
    "    print(\"Dataset preprocessed and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dataset.get_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_npmi = Coherence(texts=corpus, topk=TOP_K, processes=NUM_PROCESSES, measure='c_npmi')\n",
    "coherence_cv = Coherence(texts=corpus, topk=TOP_K, processes=NUM_PROCESSES, measure='c_v')\n",
    "coherence_umass = Coherence(texts=corpus, topk=TOP_K, processes=NUM_PROCESSES, measure='u_mass')\n",
    "coherence_uci = Coherence(texts=corpus, topk=TOP_K, processes=NUM_PROCESSES, measure='c_uci')\n",
    "\n",
    "diversity_topic = TopicDiversity(topk=TOP_K)\n",
    "diversity_kl = KLDivergence()\n",
    "\n",
    "similarity_rbo = RBO(topk=TOP_K)\n",
    "similarity_pjs = PairwiseJaccardSimilarity()\n",
    "\n",
    "significance_kluni = KL_uniform()\n",
    "\n",
    "other_metrics = [coherence_npmi, coherence_umass, coherence_uci, diversity_topic, diversity_kl, similarity_rbo, similarity_pjs, significance_kluni]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\"coherence_npmi\": coherence_npmi, \"coherence_cv\": coherence_cv, \"coherence_umass\": coherence_umass, \"coherence_uci\": coherence_uci, \"diversity_topic\": diversity_topic, \"diversity_kl\": diversity_kl, \"similarity_rbo\": similarity_rbo, \"similarity_pjs\": similarity_pjs, \"significance_kluni\": significance_kluni}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = LSI(**lsi_params)\n",
    "lda_model = LDA(**lda_params)\n",
    "hdp_model = HDP(**hdp_params)\n",
    "nmf_model = NMF(**nmf_params)\n",
    "neural_lda_model = NeuralLDA(**neural_lda_params)\n",
    "prod_lda_model = ProdLDA(**prod_lda_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"lsi\": lsi_model, \"lda\": lda_model, \"hdp\": hdp_model, \"nmf\": nmf_model, \"neural_lda\": neural_lda_model, \"prod_lda\": prod_lda_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = OCTISModelEvaluator(dataset=dataset, \n",
    "                                models=models,\n",
    "                                metrics=metrics,\n",
    "                                topics=NUM_TOPICS,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/200]\tSamples: [1439/287800]\tTrain Loss: 3378.429470335302\tTime: 0:00:00.119144\n",
      "Epoch: [1/200]\tSamples: [160/32000]\tValidation Loss: 3073.64375\tTime: 0:00:00.005158\n",
      "Epoch: [2/200]\tSamples: [2878/287800]\tTrain Loss: 3304.1669887943017\tTime: 0:00:00.107573\n",
      "Epoch: [2/200]\tSamples: [160/32000]\tValidation Loss: 3075.122900390625\tTime: 0:00:00.006745\n",
      "Epoch: [3/200]\tSamples: [4317/287800]\tTrain Loss: 3269.586035224114\tTime: 0:00:00.106692\n",
      "Epoch: [3/200]\tSamples: [160/32000]\tValidation Loss: 3068.27607421875\tTime: 0:00:00.004176\n",
      "Epoch: [4/200]\tSamples: [5756/287800]\tTrain Loss: 3251.551696056289\tTime: 0:00:00.101749\n",
      "Epoch: [4/200]\tSamples: [160/32000]\tValidation Loss: 3058.075439453125\tTime: 0:00:00.004851\n",
      "Epoch: [5/200]\tSamples: [7195/287800]\tTrain Loss: 3234.67709781098\tTime: 0:00:00.116486\n",
      "Epoch: [5/200]\tSamples: [160/32000]\tValidation Loss: 3039.773876953125\tTime: 0:00:00.005505\n",
      "Epoch: [6/200]\tSamples: [8634/287800]\tTrain Loss: 3195.0554530055592\tTime: 0:00:00.116149\n",
      "Epoch: [6/200]\tSamples: [160/32000]\tValidation Loss: 3011.08076171875\tTime: 0:00:00.005663\n",
      "Epoch: [7/200]\tSamples: [10073/287800]\tTrain Loss: 3174.2220074704655\tTime: 0:00:00.100321\n",
      "Epoch: [7/200]\tSamples: [160/32000]\tValidation Loss: 2986.735888671875\tTime: 0:00:00.005618\n",
      "Epoch: [8/200]\tSamples: [11512/287800]\tTrain Loss: 3162.890418693537\tTime: 0:00:00.092404\n",
      "Epoch: [8/200]\tSamples: [160/32000]\tValidation Loss: 2962.33203125\tTime: 0:00:00.005241\n",
      "Epoch: [9/200]\tSamples: [12951/287800]\tTrain Loss: 3150.846812022238\tTime: 0:00:00.097656\n",
      "Epoch: [9/200]\tSamples: [160/32000]\tValidation Loss: 2955.63701171875\tTime: 0:00:00.004731\n",
      "Epoch: [10/200]\tSamples: [14390/287800]\tTrain Loss: 3138.6448162786655\tTime: 0:00:00.097310\n",
      "Epoch: [10/200]\tSamples: [160/32000]\tValidation Loss: 2946.2423828125\tTime: 0:00:00.004396\n",
      "Epoch: [11/200]\tSamples: [15829/287800]\tTrain Loss: 3129.0790588082\tTime: 0:00:00.098978\n",
      "Epoch: [11/200]\tSamples: [160/32000]\tValidation Loss: 2944.62529296875\tTime: 0:00:00.004486\n",
      "Epoch: [12/200]\tSamples: [17268/287800]\tTrain Loss: 3126.7998881601807\tTime: 0:00:00.106572\n",
      "Epoch: [12/200]\tSamples: [160/32000]\tValidation Loss: 2942.1373291015625\tTime: 0:00:00.004840\n",
      "Epoch: [13/200]\tSamples: [18707/287800]\tTrain Loss: 3119.6460269718555\tTime: 0:00:00.093836\n",
      "Epoch: [13/200]\tSamples: [160/32000]\tValidation Loss: 2932.79228515625\tTime: 0:00:00.003982\n",
      "Epoch: [14/200]\tSamples: [20146/287800]\tTrain Loss: 3105.526244353718\tTime: 0:00:00.103512\n",
      "Epoch: [14/200]\tSamples: [160/32000]\tValidation Loss: 2924.9279296875\tTime: 0:00:00.004103\n",
      "Epoch: [15/200]\tSamples: [21585/287800]\tTrain Loss: 3104.326208521543\tTime: 0:00:00.095168\n",
      "Epoch: [15/200]\tSamples: [160/32000]\tValidation Loss: 2926.55341796875\tTime: 0:00:00.006708\n",
      "Epoch: [16/200]\tSamples: [23024/287800]\tTrain Loss: 3094.6043584954828\tTime: 0:00:00.094787\n",
      "Epoch: [16/200]\tSamples: [160/32000]\tValidation Loss: 2920.58232421875\tTime: 0:00:00.005735\n",
      "Epoch: [17/200]\tSamples: [24463/287800]\tTrain Loss: 3090.226318189715\tTime: 0:00:00.095838\n",
      "Epoch: [17/200]\tSamples: [160/32000]\tValidation Loss: 2919.934765625\tTime: 0:00:00.005143\n",
      "Epoch: [18/200]\tSamples: [25902/287800]\tTrain Loss: 3090.464235688846\tTime: 0:00:00.098505\n",
      "Epoch: [18/200]\tSamples: [160/32000]\tValidation Loss: 2921.2408203125\tTime: 0:00:00.004825\n",
      "Epoch: [19/200]\tSamples: [27341/287800]\tTrain Loss: 3082.7281315149407\tTime: 0:00:00.106881\n",
      "Epoch: [19/200]\tSamples: [160/32000]\tValidation Loss: 2919.9759765625\tTime: 0:00:00.007328\n",
      "Epoch: [20/200]\tSamples: [28780/287800]\tTrain Loss: 3099.006970986796\tTime: 0:00:00.113786\n",
      "Epoch: [20/200]\tSamples: [160/32000]\tValidation Loss: 2920.9345703125\tTime: 0:00:00.005529\n",
      "Epoch: [21/200]\tSamples: [30219/287800]\tTrain Loss: 3079.1841610927727\tTime: 0:00:00.106574\n",
      "Epoch: [21/200]\tSamples: [160/32000]\tValidation Loss: 2917.12177734375\tTime: 0:00:00.009859\n",
      "Epoch: [22/200]\tSamples: [31658/287800]\tTrain Loss: 3073.7775636292563\tTime: 0:00:00.127135\n",
      "Epoch: [22/200]\tSamples: [160/32000]\tValidation Loss: 2916.320166015625\tTime: 0:00:00.006515\n",
      "Epoch: [23/200]\tSamples: [33097/287800]\tTrain Loss: 3141.5432429204307\tTime: 0:00:00.119137\n",
      "Epoch: [23/200]\tSamples: [160/32000]\tValidation Loss: 2915.27197265625\tTime: 0:00:00.006357\n",
      "Epoch: [24/200]\tSamples: [34536/287800]\tTrain Loss: 3112.2678726546214\tTime: 0:00:00.127540\n",
      "Epoch: [24/200]\tSamples: [160/32000]\tValidation Loss: 2909.011279296875\tTime: 0:00:00.006053\n",
      "Epoch: [25/200]\tSamples: [35975/287800]\tTrain Loss: 3085.310051468033\tTime: 0:00:00.124277\n",
      "Epoch: [25/200]\tSamples: [160/32000]\tValidation Loss: 2910.568505859375\tTime: 0:00:00.005869\n",
      "Epoch: [26/200]\tSamples: [37414/287800]\tTrain Loss: 3080.6192397063933\tTime: 0:00:00.119363\n",
      "Epoch: [26/200]\tSamples: [160/32000]\tValidation Loss: 2913.651611328125\tTime: 0:00:00.007105\n",
      "Epoch: [27/200]\tSamples: [38853/287800]\tTrain Loss: 3065.28785180681\tTime: 0:00:00.109215\n",
      "Epoch: [27/200]\tSamples: [160/32000]\tValidation Loss: 2906.7017578125\tTime: 0:00:00.004509\n",
      "Epoch: [28/200]\tSamples: [40292/287800]\tTrain Loss: 3071.623072663308\tTime: 0:00:00.113715\n",
      "Epoch: [28/200]\tSamples: [160/32000]\tValidation Loss: 2908.84755859375\tTime: 0:00:00.004939\n",
      "Epoch: [29/200]\tSamples: [41731/287800]\tTrain Loss: 3056.569036657401\tTime: 0:00:00.109479\n",
      "Epoch: [29/200]\tSamples: [160/32000]\tValidation Loss: 2910.355615234375\tTime: 0:00:00.005941\n",
      "Epoch: [30/200]\tSamples: [43170/287800]\tTrain Loss: 3062.5833098071575\tTime: 0:00:00.116216\n",
      "Epoch: [30/200]\tSamples: [160/32000]\tValidation Loss: 2907.729736328125\tTime: 0:00:00.006440\n",
      "Epoch: [31/200]\tSamples: [44609/287800]\tTrain Loss: 3061.959487925643\tTime: 0:00:00.105331\n",
      "Epoch: [31/200]\tSamples: [160/32000]\tValidation Loss: 2906.1849609375\tTime: 0:00:00.005573\n",
      "Epoch: [32/200]\tSamples: [46048/287800]\tTrain Loss: 3053.5276504951357\tTime: 0:00:00.102730\n",
      "Epoch: [32/200]\tSamples: [160/32000]\tValidation Loss: 2909.664208984375\tTime: 0:00:00.005896\n",
      "Epoch: [33/200]\tSamples: [47487/287800]\tTrain Loss: 3053.0857420517723\tTime: 0:00:00.096547\n",
      "Epoch: [33/200]\tSamples: [160/32000]\tValidation Loss: 2905.027490234375\tTime: 0:00:00.005717\n",
      "Epoch: [34/200]\tSamples: [48926/287800]\tTrain Loss: 3062.1612230715773\tTime: 0:00:00.106928\n",
      "Epoch: [34/200]\tSamples: [160/32000]\tValidation Loss: 2906.86376953125\tTime: 0:00:00.005210\n",
      "Epoch: [35/200]\tSamples: [50365/287800]\tTrain Loss: 3051.563721551425\tTime: 0:00:00.111726\n",
      "Epoch: [35/200]\tSamples: [160/32000]\tValidation Loss: 2903.93271484375\tTime: 0:00:00.004169\n",
      "Epoch: [36/200]\tSamples: [51804/287800]\tTrain Loss: 3078.662515201529\tTime: 0:00:00.131555\n",
      "Epoch: [36/200]\tSamples: [160/32000]\tValidation Loss: 2900.6296875\tTime: 0:00:00.006029\n",
      "Epoch: [37/200]\tSamples: [53243/287800]\tTrain Loss: 3050.8167347116055\tTime: 0:00:00.117392\n",
      "Epoch: [37/200]\tSamples: [160/32000]\tValidation Loss: 2903.821826171875\tTime: 0:00:00.005817\n",
      "Epoch: [38/200]\tSamples: [54682/287800]\tTrain Loss: 3047.5315377432244\tTime: 0:00:00.101886\n",
      "Epoch: [38/200]\tSamples: [160/32000]\tValidation Loss: 2904.31162109375\tTime: 0:00:00.004420\n",
      "Epoch: [39/200]\tSamples: [56121/287800]\tTrain Loss: 3044.6058922211605\tTime: 0:00:00.108271\n",
      "Epoch: [39/200]\tSamples: [160/32000]\tValidation Loss: 2897.91337890625\tTime: 0:00:00.004617\n",
      "Epoch: [40/200]\tSamples: [57560/287800]\tTrain Loss: 3068.2855878648365\tTime: 0:00:00.094515\n",
      "Epoch: [40/200]\tSamples: [160/32000]\tValidation Loss: 2898.171337890625\tTime: 0:00:00.004451\n",
      "Epoch: [41/200]\tSamples: [58999/287800]\tTrain Loss: 3048.6748610145933\tTime: 0:00:00.100155\n",
      "Epoch: [41/200]\tSamples: [160/32000]\tValidation Loss: 2901.03818359375\tTime: 0:00:00.004717\n",
      "Epoch: [42/200]\tSamples: [60438/287800]\tTrain Loss: 3045.250008143676\tTime: 0:00:00.102035\n",
      "Epoch: [42/200]\tSamples: [160/32000]\tValidation Loss: 2898.2083984375\tTime: 0:00:00.005505\n",
      "Epoch: [43/200]\tSamples: [61877/287800]\tTrain Loss: 3047.5548666608756\tTime: 0:00:00.111727\n",
      "Epoch: [43/200]\tSamples: [160/32000]\tValidation Loss: 2901.179248046875\tTime: 0:00:00.006214\n",
      "Epoch: [44/200]\tSamples: [63316/287800]\tTrain Loss: 3043.1120678422517\tTime: 0:00:00.096553\n",
      "Epoch: [44/200]\tSamples: [160/32000]\tValidation Loss: 2899.45439453125\tTime: 0:00:00.006006\n",
      "Early stopping\n",
      "Epoch: [1/50]\tSamples: [1439/71950]\tTrain Loss: 3531.966306897151\tTime: 0:00:00.076541\n",
      "Epoch: [1/50]\tSamples: [160/8000]\tValidation Loss: 24858.79013671875\tTime: 0:00:00.004326\n",
      "Epoch: [2/50]\tSamples: [2878/71950]\tTrain Loss: 3361.0913991921475\tTime: 0:00:00.080650\n",
      "Epoch: [2/50]\tSamples: [160/8000]\tValidation Loss: 3104.27802734375\tTime: 0:00:00.006235\n",
      "Epoch: [3/50]\tSamples: [4317/71950]\tTrain Loss: 3280.2387074357193\tTime: 0:00:00.085793\n",
      "Epoch: [3/50]\tSamples: [160/8000]\tValidation Loss: 3063.47001953125\tTime: 0:00:00.003942\n",
      "Epoch: [4/50]\tSamples: [5756/71950]\tTrain Loss: 3257.385342468728\tTime: 0:00:00.081159\n",
      "Epoch: [4/50]\tSamples: [160/8000]\tValidation Loss: 3038.532373046875\tTime: 0:00:00.004772\n",
      "Epoch: [5/50]\tSamples: [7195/71950]\tTrain Loss: 3212.844672949965\tTime: 0:00:00.078100\n",
      "Epoch: [5/50]\tSamples: [160/8000]\tValidation Loss: 3016.867236328125\tTime: 0:00:00.004489\n",
      "Epoch: [6/50]\tSamples: [8634/71950]\tTrain Loss: 3218.5498175816538\tTime: 0:00:00.072543\n",
      "Epoch: [6/50]\tSamples: [160/8000]\tValidation Loss: 2994.148291015625\tTime: 0:00:00.005975\n",
      "Epoch: [7/50]\tSamples: [10073/71950]\tTrain Loss: 3203.385195882557\tTime: 0:00:00.078933\n",
      "Epoch: [7/50]\tSamples: [160/8000]\tValidation Loss: 2986.506201171875\tTime: 0:00:00.005688\n",
      "Epoch: [8/50]\tSamples: [11512/71950]\tTrain Loss: 3200.683895066018\tTime: 0:00:00.088067\n",
      "Epoch: [8/50]\tSamples: [160/8000]\tValidation Loss: 2990.330712890625\tTime: 0:00:00.005546\n",
      "Epoch: [9/50]\tSamples: [12951/71950]\tTrain Loss: 3196.913286136206\tTime: 0:00:00.081338\n",
      "Epoch: [9/50]\tSamples: [160/8000]\tValidation Loss: 2980.55966796875\tTime: 0:00:00.004392\n",
      "Epoch: [10/50]\tSamples: [14390/71950]\tTrain Loss: 3178.5496275625433\tTime: 0:00:00.090559\n",
      "Epoch: [10/50]\tSamples: [160/8000]\tValidation Loss: 2976.66513671875\tTime: 0:00:00.003664\n",
      "Epoch: [11/50]\tSamples: [15829/71950]\tTrain Loss: 3177.792569709868\tTime: 0:00:00.086266\n",
      "Epoch: [11/50]\tSamples: [160/8000]\tValidation Loss: 2984.3818359375\tTime: 0:00:00.005418\n",
      "Epoch: [12/50]\tSamples: [17268/71950]\tTrain Loss: 3169.485232800556\tTime: 0:00:00.091054\n",
      "Epoch: [12/50]\tSamples: [160/8000]\tValidation Loss: 2970.59501953125\tTime: 0:00:00.005396\n",
      "Epoch: [13/50]\tSamples: [18707/71950]\tTrain Loss: 3163.3070600243223\tTime: 0:00:00.085935\n",
      "Epoch: [13/50]\tSamples: [160/8000]\tValidation Loss: 2964.79013671875\tTime: 0:00:00.006843\n",
      "Epoch: [14/50]\tSamples: [20146/71950]\tTrain Loss: 3183.0496058460735\tTime: 0:00:00.089469\n",
      "Epoch: [14/50]\tSamples: [160/8000]\tValidation Loss: 2971.966357421875\tTime: 0:00:00.004203\n",
      "Epoch: [15/50]\tSamples: [21585/71950]\tTrain Loss: 3169.385413047255\tTime: 0:00:00.089112\n",
      "Epoch: [15/50]\tSamples: [160/8000]\tValidation Loss: 2983.28955078125\tTime: 0:00:00.005766\n",
      "Epoch: [16/50]\tSamples: [23024/71950]\tTrain Loss: 3176.1583673558025\tTime: 0:00:00.086400\n",
      "Epoch: [16/50]\tSamples: [160/8000]\tValidation Loss: 2968.52490234375\tTime: 0:00:00.005588\n",
      "Epoch: [17/50]\tSamples: [24463/71950]\tTrain Loss: 3156.7435990705353\tTime: 0:00:00.082264\n",
      "Epoch: [17/50]\tSamples: [160/8000]\tValidation Loss: 2963.62744140625\tTime: 0:00:00.004066\n",
      "Epoch: [18/50]\tSamples: [25902/71950]\tTrain Loss: 3179.048541739055\tTime: 0:00:00.092597\n",
      "Epoch: [18/50]\tSamples: [160/8000]\tValidation Loss: 2976.13349609375\tTime: 0:00:00.005541\n",
      "Epoch: [19/50]\tSamples: [27341/71950]\tTrain Loss: 3165.2278329134815\tTime: 0:00:00.092198\n",
      "Epoch: [19/50]\tSamples: [160/8000]\tValidation Loss: 2970.9890625\tTime: 0:00:00.005092\n",
      "Epoch: [20/50]\tSamples: [28780/71950]\tTrain Loss: 3174.2446794649063\tTime: 0:00:00.076761\n",
      "Epoch: [20/50]\tSamples: [160/8000]\tValidation Loss: 2967.92216796875\tTime: 0:00:00.003720\n",
      "Epoch: [21/50]\tSamples: [30219/71950]\tTrain Loss: 3154.4092577310635\tTime: 0:00:00.072797\n",
      "Epoch: [21/50]\tSamples: [160/8000]\tValidation Loss: 2975.26044921875\tTime: 0:00:00.006867\n",
      "Epoch: [22/50]\tSamples: [31658/71950]\tTrain Loss: 3154.1419822793605\tTime: 0:00:00.077790\n",
      "Epoch: [22/50]\tSamples: [160/8000]\tValidation Loss: 2964.469287109375\tTime: 0:00:00.004654\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dion/Library/CloudStorage/GoogleDrive-dion.rigatos@gmail.com/My Drive/Archivio/University/Classes/Erasmus Courses/NLP/NLP Project/greek-pm-topic-modeling/src/models/octis/utils/model_evaluator.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.evaluation_df = pd.concat([self.evaluation_df, pd.DataFrame(model_metric_data)], ignore_index=True)\n",
      "/Users/dion/.pyenv/versions/3.11.8/envs/nlp-env/lib/python3.11/site-packages/octis/evaluation_metrics/diversity_metrics.py:244: RuntimeWarning: invalid value encountered in log\n",
      "  divergence = np.sum(P*np.log(P/Q))\n",
      "/Users/dion/.pyenv/versions/3.11.8/envs/nlp-env/lib/python3.11/site-packages/octis/evaluation_metrics/topic_significance_metrics.py:24: RuntimeWarning: invalid value encountered in log\n",
      "  divergence = np.sum(P*np.log(P/Q))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>coherence_npmi</th>\n",
       "      <th>coherence_cv</th>\n",
       "      <th>coherence_umass</th>\n",
       "      <th>coherence_uci</th>\n",
       "      <th>diversity_topic</th>\n",
       "      <th>diversity_kl</th>\n",
       "      <th>similarity_rbo</th>\n",
       "      <th>similarity_pjs</th>\n",
       "      <th>significance_kluni</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lsi</td>\n",
       "      <td>0.124227</td>\n",
       "      <td>0.663027</td>\n",
       "      <td>-1.205847</td>\n",
       "      <td>0.609450</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.885864</td>\n",
       "      <td>0.056632</td>\n",
       "      <td>0.034798</td>\n",
       "      <td>0.434231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lda</td>\n",
       "      <td>0.101787</td>\n",
       "      <td>0.656160</td>\n",
       "      <td>-1.101430</td>\n",
       "      <td>0.298030</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>1.811897</td>\n",
       "      <td>0.016060</td>\n",
       "      <td>0.010660</td>\n",
       "      <td>1.310394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hdp</td>\n",
       "      <td>-0.051897</td>\n",
       "      <td>0.509259</td>\n",
       "      <td>-2.202071</td>\n",
       "      <td>-2.674711</td>\n",
       "      <td>0.554667</td>\n",
       "      <td>0.362610</td>\n",
       "      <td>0.016090</td>\n",
       "      <td>0.012933</td>\n",
       "      <td>0.214266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nmf</td>\n",
       "      <td>0.181904</td>\n",
       "      <td>0.699247</td>\n",
       "      <td>-1.021196</td>\n",
       "      <td>0.953869</td>\n",
       "      <td>0.723077</td>\n",
       "      <td>3.942570</td>\n",
       "      <td>0.054839</td>\n",
       "      <td>0.055419</td>\n",
       "      <td>2.125015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neural_lda</td>\n",
       "      <td>0.035713</td>\n",
       "      <td>0.619024</td>\n",
       "      <td>-1.528981</td>\n",
       "      <td>-1.075232</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>prod_lda</td>\n",
       "      <td>-0.093289</td>\n",
       "      <td>0.559137</td>\n",
       "      <td>-1.808297</td>\n",
       "      <td>-4.493539</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009805</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model  coherence_npmi  coherence_cv  coherence_umass  coherence_uci  \\\n",
       "0         lsi        0.124227      0.663027        -1.205847       0.609450   \n",
       "1         lda        0.101787      0.656160        -1.101430       0.298030   \n",
       "2         hdp       -0.051897      0.509259        -2.202071      -2.674711   \n",
       "3         nmf        0.181904      0.699247        -1.021196       0.953869   \n",
       "4  neural_lda        0.035713      0.619024        -1.528981      -1.075232   \n",
       "5    prod_lda       -0.093289      0.559137        -1.808297      -4.493539   \n",
       "\n",
       "   diversity_topic  diversity_kl  similarity_rbo  similarity_pjs  \\\n",
       "0         0.738462      0.885864        0.056632        0.034798   \n",
       "1         0.907692      1.811897        0.016060        0.010660   \n",
       "2         0.554667      0.362610        0.016090        0.012933   \n",
       "3         0.723077      3.942570        0.054839        0.055419   \n",
       "4         1.000000      0.978301        0.000000        0.000000   \n",
       "5         0.923077           NaN        0.009805        0.004049   \n",
       "\n",
       "   significance_kluni  \n",
       "0            0.434231  \n",
       "1            1.310394  \n",
       "2            0.214266  \n",
       "3            2.125015  \n",
       "4            0.495215  \n",
       "5                 NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
