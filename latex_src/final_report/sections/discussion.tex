\section{Evaluation and Discussion}
\label{sec:Discussion}

Going into this task, we were aware of the challenges when it comes to evaluating topic models - the results, however, are even more concerning than what we initially expected. As we see in Table \ref{tab:quant_metrics}, the quantitative evaluation of in our models seems to be especially confusing. We made the choice of tuning our models based on CV Coherence, which in retrospect appears to be far from the ideal coherence measure - raising concerns about whether our optimization achieved the best possible, and interpretable, results. However, this also seems to be the case with UMass. According to our empirical evaluation based on Table \ref{tab:best_model_topics}, BERTopic+ and ProdLDA produced the best topics. This is where the incoherence of coherence starts - with ProdLDA having the worst UMass and the best CV coherence scores, and BERTopic+ having a CV score that we would, unless we had seen the topics, discard immediately. On the other hand, Topic Diversity and Pairwise-Jaccard Similarity seem to be directly correlated with our empirical evaluation. 

Qualitative results are presented in Table \ref{tab:best_model_topics}, where the strictness of the ProdLDA preprocessing is apparent due to the absence of irrelevant words and the diversity of topics. On the other hand, BERTopic seems to suffer from poor topic representations. This is visible on Topic 4, with the words "que, de, en, la, el" being the prime representation words for that topic. BERTopic+ has eliminated such noise and was able to produce richer topics. On the analytical side, as expected, all of the algorithms produced topics related to economy, external policies, healthcare, infrastructure, democracy, COVID-19, immigration and other political cliches. For brevity reasons, not all topics are shown, with more niche topics being shadowed. Finally, Figure \ref{fig:doc_topic_plot} presents how the top topic clusters of BERTopic+ are distributed. They are clearly distinct with no overlap, something attributed to the optimized UMAP dimensionality reduction as well as the HDBSCAN clustering. It is safe to say that our top-performing models, in combination with our preliminary work, are able to achieve notable results.

